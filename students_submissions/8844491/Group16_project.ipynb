{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><b>Object Detection on COVID-19 Dataset</b></center>\n",
    "<center>Done by: Group 16</center>\n",
    "<center><br>Aardran Premakumar - 8844491</center>\n",
    "<center><br> Meenu Ramesh - 8945753</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Dataset Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Selection\n",
    "\n",
    "# Set the path to the dataset and labels CSV files\n",
    "dataset_path = r\"C:\\Users\\aardr\\OneDrive\\CONESTOGA\\AI & ML\\Sem1\\Foundations of ML\\Project\\Datasets\\COVID-19 PPE data\\dataset\"\n",
    "train_labels_path = r\"C:\\Users\\aardr\\OneDrive\\CONESTOGA\\AI & ML\\Sem1\\Foundations of ML\\Project\\Datasets\\COVID-19 PPE data\\tf_record_files\\train_labels.csv\"\n",
    "test_labels_path = r\"C:\\Users\\aardr\\OneDrive\\CONESTOGA\\AI & ML\\Sem1\\Foundations of ML\\Project\\Datasets\\COVID-19 PPE data\\tf_record_files\\test_labels.csv\"\n",
    "\n",
    "# Paths for train and test datasets\n",
    "train_path = os.path.join(dataset_path, \"train\")\n",
    "test_path = os.path.join(dataset_path, \"test\")\n",
    "\n",
    "\n",
    "# Load labels from CSV files\n",
    "train_labels_df = pd.read_csv(train_labels_path)\n",
    "test_labels_df = pd.read_csv(test_labels_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique classes =  5\n"
     ]
    }
   ],
   "source": [
    "# find the number of classes\n",
    "train_labels = train_labels_df['class'].values\n",
    "a = pd.Series(train_labels).unique()\n",
    "c = pd.Series(a).value_counts()\n",
    "num_classes = len(c)\n",
    "print(\"The number of unique classes = \", num_classes) # number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Images on train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# TODO: Implement function to preprocess images and extract bounding boxes\n",
    "def preprocess_images(image_path, labels):\n",
    "    # load image, resize, and extract bounding boxes\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((640, 640))\n",
    "    image = np.array(image)  # convert image to NumPy array\n",
    "    image = torch.tensor(image)\n",
    "    # Check the shape of the image tensor\n",
    "    if len(image.shape) < 3:\n",
    "        image = image.unsqueeze(0)  # Add a dimension for channels if it's missing\n",
    "    \n",
    "    image = image.permute(2, 0, 1)  # Permute the dimensions\n",
    "    image = image.float()\n",
    "    image = image / 255.0\n",
    "    # extract bounding boxes from labels\n",
    "    bounding_boxes = []\n",
    "    for label in labels:\n",
    "        if label[0] == image_path:\n",
    "            bounding_boxes.append(label[1:])\n",
    "    return image, bounding_boxes\n",
    "\n",
    "# preprocess train and test images\n",
    "train_images = []\n",
    "train_bounding_boxes = []\n",
    "for image_path in train_labels_df[\"filename\"]:\n",
    "    image, bounding_boxes = preprocess_images(os.path.join(train_path, image_path), train_labels_df.values)\n",
    "    train_images.append(image)\n",
    "    train_bounding_boxes.append(bounding_boxes)\n",
    "\n",
    "test_images = []\n",
    "test_bounding_boxes = []\n",
    "for image_path in test_labels_df[\"filename\"]:\n",
    "    image, bounding_boxes = preprocess_images(os.path.join(test_path, image_path), test_labels_df.values)\n",
    "    test_images.append(image)\n",
    "    test_bounding_boxes.append(bounding_boxes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model (YOLO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\aardr/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-12-14 Python-3.10.5 torch-2.1.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Load the pre-trained YOLOv5 model\n",
    "model_yaml = r'C:\\Users\\aardr\\OneDrive\\CONESTOGA\\AI & ML\\Sem1\\Foundations of ML\\Project\\yolov5\\models\\yolov5s.yaml'\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True) # yolov5s is the smallest\n",
    "\n",
    "\n",
    "# Reference: https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the YOLO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customizing the model\n",
    "my_model = model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5  # number of classes in the dataset\n",
    "my_model.nc = num_classes  # number of classes in dataset\n",
    "my_model.names = ['Hard Hat', 'Mask', 'Vest', 'Boots', 'Gloves']  # class names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing some layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze first few layers\n",
    "for param in my_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(my_model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: There was a RunTimeError on the next code cell, so to ensure image should have 3 channels.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "def resize_image(image):\n",
    "    # Ensure image has 3 channels (RGB)\n",
    "    if image.shape[0] == 4:  # If the image has 4 channels (e.g., RGBA)\n",
    "        image = image[:3, :, :]  # Keep only the first 3 channels (RGB)\n",
    "    elif image.shape[0] == 1:  # If the image has 1 channel (e.g., grayscale)\n",
    "        image = image.repeat(3, 1, 1)  # Repeat the channel to create a 3-channel image\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = F.interpolate(image.unsqueeze(0), size=(640, 640), mode='bilinear', align_corners=False).squeeze(0)\n",
    "    return resized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch Images Shape: torch.Size([32, 3, 640, 640])\n",
      "Resized Image Shape: torch.Size([3, 640, 640])\n",
      "O/P shape torch.Size([33, 25200, 85])\n",
      "Loss: 0.0000\n",
      "Batch Images Shape: torch.Size([32, 3, 640, 640])\n",
      "Resized Image Shape: torch.Size([3, 640, 640])\n",
      "O/P shape torch.Size([33, 25200, 85])\n",
      "Loss: 0.0000\n",
      "Batch Images Shape: torch.Size([32, 3, 640, 640])\n",
      "Resized Image Shape: torch.Size([3, 640, 640])\n",
      "O/P shape torch.Size([33, 25200, 85])\n",
      "Loss: 0.0000\n",
      "Batch Images Shape: torch.Size([32, 3, 640, 640])\n",
      "Resized Image Shape: torch.Size([3, 640, 640])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "    for i in range(0, len(train_images), batch_size):\n",
    "        batch_images = train_images[i:i+batch_size]\n",
    "        \n",
    "        loss = torch.tensor(0, requires_grad=True, dtype=torch.float32)  # Initialize loss as a tensor\n",
    "        \n",
    "        # FILEPATH: Untitled-3.ipynb\n",
    "        batch_bounding_boxes = train_bounding_boxes[i:i+batch_size]\n",
    "        # Resize bounding boxes to match the number of output channels\n",
    "        batch_bounding_boxes = [boxes[:3] for boxes in batch_bounding_boxes]\n",
    "        print(f\"Batch Images Shape: {torch.stack(batch_images).shape}\")\n",
    "\n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "        \n",
    "        if batch_bounding_boxes:  # Check if bounding_boxes is not empty\n",
    "            # Resize images to ensure they have the same dimensions\n",
    "            resized_images = []\n",
    "            for image in batch_images:\n",
    "                resized_image = resize_image(image)\n",
    "                resized_images.append(resized_image)\n",
    "            resized_images.append(resized_image)\n",
    "            print(f\"Resized Image Shape: {resized_image.shape}\")  # Debug print\n",
    "            resized_images = torch.stack(resized_images)  # Stack the resized images\n",
    "\n",
    "            outputs = my_model(resized_images)\n",
    "            print(\"O/P shape\", outputs.shape)\n",
    "\n",
    "            for output, bounding_boxes in zip(outputs, batch_bounding_boxes):\n",
    "                if len(bounding_boxes) > 0:  # Check if bounding_boxes is not empty\n",
    "                    loss += criterion(output, torch.tensor(bounding_boxes, dtype=torch.float32))\n",
    "\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# Refernce: https://www.cs.toronto.edu/~lczhang/360/lec/w02/training.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Couldn't complete the training since it's raising error.\n",
    "\n",
    "`RuntimeError: stack expects each tensor to be equal size, but got [3, 640, 640] at entry 0 and [4, 640, 640] at entry 14`\n",
    "\n",
    "Have been stuck on it, and couldn't debug it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This project was about trying YOLO (You Only Look Once) for finding objects. We started really excited because YOLO is known for being fast and good at spotting objects. But soon, we faced many hard problems that made our work slow. It felt like fixing one thing just caused another problem, showing how tough it is to work with object detection. Even though we didn't get the results we wanted, we learned a lot about the real challenges of doing object detection, which helps us get ready for more work in this interesting area."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "tensorflow_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
